# HDFS Datalake

In this practice we will setup a datalake on HDFS and Hive environment.

![Docker-Python](img/data-lake.jpg)

## Prerequisites

* Follow the [pre-setup guideline][pre-setup]

## Before start

Let's review some concepts we will use during this session:

### Datalake

>Is a centralized repository that allows you to `store` and `manage large volumes` of `structured`, `semi-structured`, and `unstructured data`.

#### Where can be implemented a Datalake?

* `HDFS with Hive` \
  This is probably one of the most common datalakes.
  * `HDFS` provide a distributed file system
  * `Hive` provides a SQL-like interface
* `S3 with Athena` \
  AWS Cloud based
  * `S3` AWS filesystem
  * `Athena` serverless query service (SQL like)
* `Azure Blob Storage with Azure Data Lake Analytics` \
  Microsoft Azure cloud based
  * Azure `Blob Storage` provides the volumes
  * Azure `Data Lake Analytics` is a serverless analytics service
* `Google Cloud Storage with BigQuery` \
  GCP cloud based
  * `Google Cloud Storage` works as storage
  * `BigQuery` is a fully-managed data warehouse service (SQL queries)

  ![Docker-Python](img/data-lake-6.png)

>In this practice we will focus on `Hive/HDFS`

### Apache Hive

Apache Hive is a tool that is built on top of Hadoop and is designed to allow you to query and analyze data stored in HDFS or other distributed storage systems.

>Hive provides a SQL-like language called `HiveQL` to query data

* `Schemas for files stored on HDFS` \
  When you create a table in Hive, you define the schema of the table, including the column names, data types, and any other metadata that is required. *Hive then maps the columns in the table to the columns in the underlying data files stored on HDFS*, based on the file format and the data in the files.
* Format support \
  Hive **supports a wide range of file formats, including CSV, JSON, Avro, ORC, and Parquet**. Each file format has its own way of storing data, and Hive is able to interpret the data in the files and create schemas for the tables based on the file format.

For example, if you have a CSV file stored on HDFS that contains customer data, you can create a table in Hive that maps to the columns in the CSV file. Here is an example HiveQL statement that creates a table based on a CSV file stored on HDFS:

```sql
CREATE TABLE customers (
    id INT,
    name STRING,
    email STRING,
    age INT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION '/path/to/csv/file';
```

#### Apache Hive Components

* `Hive Metastore` \
  Central repository, stores (*metadata*) information about Hive tables, including the table schema, column names, data types, and partitioning information. \
  > The Metastore stores this metadata in a relational database, such as MySQL, Oracle, or PostgreSQL. By separating the metadata from the actual data stored on HDFS, Hive is able to provide a level of abstraction that allows users to query and analyze data using SQL-like syntax, without having to worry about the underlying storage system.
* `Hive Query Processor` \
  Transforms `HiveQL` queries into `MapReduce or Tez jobs` for `Hadoop cluster`.
  >The Query Processor parses the HiveQL queries, performs optimization, and generates a query plan that is optimized for distributed processing.
* `Hive Execution Engine` \
  Executes the `MapReduce` or `Tez jobs` generated by the Query Processor.
* `Hive CLI and Beeline`
  Both are `command-line`.
  * Hive CLI is for `shell` results
  * Beeline uses `JDBC`, so it can be used with external tools and apps.
* `Hive Drivers and APIs` \
  Include JDBC, ODBC, Thrift..

  ![Docker-Python](img/data-lake-10.jpeg)

### Data Lake on HDFS/Hive

>This is not the only method to mount a Data Lake

1. `Ingest` \
  The first step is to introduce the data into the HDFS.
    >This can be done using a variety of tools, including Apache Kafka, Apache Flume, and Apache NiFi.
2. `Define a schema` \
  Using Hive you can set the schema
    >This creates a table that defines the structure of the data, including the column names, data types, and any other metadata that is required

___
At this step the Data Lake is already mounted, but usually there is an extra step

* `Using the Data Lake` \
  >Using HiveQL you can query the Data Lake.
* `Iterate and refine` \
  Hive allows you to easily modify the schema as needed, without having to manually modify the data stored in HDFS.
  >After some time using the Data Lake and working with the data, you may discover that the schema needs to be refined or updated to better support your analysis.

  ![Docker-Python](img/data-lake-8.png)

## What You Will Learn

* Datalake Concepts and Components
* Hive
* HDFS
* Implement a Datalake on HDFS and Hive

## Practice

You're working on a global e-commerce company, and the company recieves multiple CSV and JSON files every day about the products that has been sold.

![img](img/data-lake-2.jpeg)

### Requirements

* Create a Datalake using `Hive/HDFS`
  * Consider hdfs directory structure that supports `daily` inputs
    * Consider `raw`, `pre-processed` and `processed` folders in your structure
  * Upload the sample files
  * Create the folder structure on the HDFS
    * Upload sample files to folder structure
  * Create hive tables from sample files

Consider the samples `products_1.csv` and `products_1a.csv` are from `Jan 01 2023` and `products_2.csv` is from  `Jan 02 2023`

#### Step 1 - Data lake filesystem

* Consider `raw`, `pre-processed` and `processed` folders in your structure

  >When you work with multiple data stages (`raw`, `pre-processed`, and `processed`) is a good idea to split them.

  The most common approach is to use a hierarchical directory structure

  ```txt
  <root>
    raw
    pre-processed
    processed
  ```

* Consider hdfs directory structure that supports `daily` inputs

  ```txt
  <root>
    processed
      year=2022
        month=01
          day=01
            data1_processed.csv
            data2_processed.csv
          day=02
            ...
        month=02
          ...
      year=2023
        ...
  ```

### Step 2 - Upload files into HDFS

* Upload the sample files
  * Copy the folder `raw_files` in the [namenode][namenode] container

    ```sh
    docker cp ./raw_files practice_files-namenode-1:/tmp
    ```

### Step 3 - HDFS folder structure

* Create the folder structure on the HDFS
  * Connect to `namenode` container and navigate to uploaded folder

    ```sh
    # Open docker container
    docker-compose exec namenode bash

    # Move to directory
    cd /tmp/raw_files/
    ```

  * Create the folders for samples csv using the structure from step 1

    ```sh
    hdfs dfs -mkdir -p /raw/year=2023/month=01/day=01/
    hdfs dfs -mkdir -p /raw/year=2023/month=01/day=02/
    ```

* Upload sample files to folder structure

  ```sh
  # `products_1.csv` and `products_1a.csv` are from `Jan 01 2023`
  hdfs dfs -put products_1.csv /raw/year=2023/month=01/day=01/
  hdfs dfs -put products_1a.csv /raw/year=2023/month=01/day=01/

  # `products_2.csv` is from `Jan 02 2023`
  hdfs dfs -put products_2.csv /raw/year=2023/month=01/day=02/
  ```

* Exit the container with `exit`

### Step 3 - Hive tables

* Create hive tables from sample files
  * Connect to hive-metastore service container with following command:

    ```sh
    docker-compose exec hive-metastore bash
    ```

  * Connect to Hive

    ```sh
    hive
    ```

  * Create new database

    ```sh
    create database products_db;
    ```

  * Verify the database is created

    ```sh
    show databases;
    ```

  * Select the database we will be using

    ```sh
    use products_db;
    ```

  * Create `products`

    ```sql
    CREATE EXTERNAL TABLE products (
      id INT,
      product_name STRING,
      product_price INT,
      product_country STRING,
      number_sold_per_day INT
    )
    PARTITIONED BY (year INT, month INT, day INT)
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ','
    STORED AS TEXTFILE
    LOCATION '/raw/';
    ```

  * Check the records on product table

    ```sql
    SELECT * FROM products;
    ```

    >There is no data, this is because we need to add the partitions to the directories we created to load the data

  * Add partitions

    ```sql
    MSCK REPAIR TABLE products;
    ```

  * Check the records on product table

    ```sql
    SELECT * FROM products;
    ```

>We can filter the data even by year, month and day.
>
>Ex. if we like the data from (year=2023, month=01, day=02), we can do the following SQL statement:

```sql
SELECT * FROM products WHERE year='2023' AND month='01' AND day='01';
```

## Conclusion

In summary, a datalake is a centralized repository for storing and managing large volumes of data, and it can be implemented using HDFS and Hive to provide a scalable and fault-tolerant storage and querying solution for big data analytics and data science projects.

## Still curious

Some other Hive features are:

* Dynamically add partitions
  * [Article][article_partitions]
  * You can connect to Hive within hive-metastore with following command:

    ```sh
    /opt/hive/bin/beeline -u jdbc:hive2://localhost:10000
    ```

* There are other drivers to connect Hive, presto is one of them:

    ```sh
    ./presto.jar --server localhost:8080 --catalog hive --schema default
    ```

Why Hive is important?

Hive is now one of the most widely used technologies for big data analysis, there are a couple reasons, the most importants are:

* Easy to understand: Since it uses SQL
* High Fault Tolerance
* Integration with HDFS: Mount over existing technologies you may already use
* Cheap: Since you may already have the HDFS datalake you don't need additional infrastructure

Wanna know more and some AirBnB case studies?

* Article: [What is Hive Big Data and its Benefits?][article_hive_benefits]

## Links

### Used during this session

* [Pre-Setup][pre-setup]

* [Dynamically add partitions with Hive][article_partitions]

### Session reinforment and homework help

* [What Is a Data Lake? Types, Elements & Best Practices][data_lake_best_practices]
* [Docker-hive][docker_hive]
* Documentation: [Hive Quickstart][hive_quickstart]
* [Official Documentation Hive][hive_docs]
* [Official Documentation HiveQL][hiveql]

[pre-setup]: ./pre-setup.md
[namenode]: #apache-hive-components

[article_partitions]: https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.0.0/using-hiveql/content/hive-create_partitions_dynamically.html
[article_hive_benefits]: https://www.analyticssteps.com/blogs/what-hive-big-data-and-its-benefits

[data_lake_best_practices]: https://www.netsuite.com/portal/resource/articles/data-warehouse/data-lake.shtml
[docker_hive]: https://github.com/big-data-europe/docker-hive
[hive_quickstart]: https://hive.apache.org/developement/quickstart/
[hive_docs]: https://hive.apache.org/docs/
[hiveql]: https://docs.hivesql.io/tutorials/hivesql-for-python-developers
